\chapter{Mô hình Graph Attention Network}
\label{chap:gat-architecture}


Trong phần này, chúng tôi sẽ trình bày các kiến thức nền tảng được sử dụng để xây dựng các mạng quan sát đồ thị tùy chọn (bằng cách gắp lớp này) và liệt kê các hiệu quả về mặt lý thuyết và thực tế cùng các hạn chế so với công trình liên quan đến lĩnh vực xử lý đồ thị nơ-ron trước đây.



% -------------------------------------------------------------------
% Đồ Thị Phân Lớp Tập Trung (Graph Attentional Layer)
% -------------------------------------------------------------------

\section{Đồ Thị Phân Lớp Tập Trung (Graph Attentional Layer)}
Chúng ta bắt đầu bằng viết một phân lớp đồ thị tập trung đơn lẻ, đây là lớp duy nhất được sử dụng trong toàn bộ các mô hình GAT trong các thử nghiệm của chúng tôi. Thiết kế chú ý cụ thể được chúng tôi sử dụng gần gũi với công trình của Bahdanau et al. (2015) - nhưng khung làm việc là agnostic với lựa chọn cụ thể của mức độ chú ý.

Đầu vào của lớp của chúng tôi là một tập hợp các đặc điểm nút, trong đó N là số lượng nút và F là số lượng đặc điểm trong mỗi nút. Lớp tạo ra một tập hợp các đặc điểm nút mới (có thể khác với số lượng F), như đầu ra của nó.

Để có đủ năng lượng biểu diễn để chuyển đổi đặc trưng đầu vào thành đặc trưng cấp cao hơn, ít nhất một biến đổi tuyến tính có thể học là cần thiết. Để đạt được mục đích đó, làm một bước đầu tiên, một biến đổi tuyến tính chung, được tham số hóa bởi ma trận trọng số  ..., được áp dụng cho mỗi nút. Chúng ta sau đó thực hiện tự quan tâm trên các nút - một mục đích quan tâm chung …. tính hệ số chú ý
….
cho thấy sự quan trọng của đặc trưng của nút j đối với nút i. Trong biểu diễn tổng quát nhất, mô hình cho phép mỗi nút chú ý tới mọi nút khác, loại bỏ tất cả thông tin cấu trúc. Chúng ta tiếp cận cấu trúc đồ thị qua việc thực hiện sự chú ý được che đậy - chúng ta chỉ tính toán sự chú ý được che đậy -Chỉ tính eij cho các nút j  Ni, trong đó Ni là một khu vực xung quanh nút i trong đồ thị. Trong tất cả các thí nghiệm của chúng tôi, đó sẽ chính là các nút đồng bậc đầu tiên của i (bao gồm i). Để dễ dàng so sánh các hệ số giữa các nút khác nhau, chúng tôi chuẩn hóa chúng theo tất cả các lựa chọn của j bằng hàm softmax:

…..
Trong các thí nghiệm của chúng tôi, cơ chế chú ý a là một mạng nơ-ron feedforward đơn tầng, được tham số hóa bởi một vectơ trọng số... và áp dụng phi tuyến LeakyReLU (với đầu vào dối mạng âm  0,2). Đủ mở rộng ra, các hệ số được tính bởi cơ chế chú ý (minh họa bởi Hình 1 (trái)) có thể được biểu diễn như sau:


% -------------------------------------------------------------------
% So sánh với các nghiên cứu trước
% -------------------------------------------------------------------

\section{So sánh với các nghiên cứu trước}

Aspect-Based Sentiment Analysis (ABSA) is a sub-field of sentiment analysis, which allows us to deeply understand and determine sentiment in terms of different aspects of the topic (Thin et al., 2018 \cite{van2018transformation}). An ABSA system receives textual data (e.g., reviews or comments on shopping platforms) about specific entity (e.g., baby care products like diapers). The system must be able to detect the mainly discussed aspects of the entity (e.g., ``\texttt{quality}'', ``\texttt{delivery}''), together with the sentiment of each aspects, or how positive/negative the opinions are. So basically the ABSA problem can be divided to two sub-task: aspect detection and sentiment polarity detection. The first sub-task attempts to determine all aspects from opinion, meanwhile the second sub-task aims to decide which sentiment polarity each aspect is. Considering above example "Soft, thin, good quality diapers, absorb a lot but delivery is quite slow", the system must be able to determine all the aspect-sentiment tuple: \{\texttt{quality}, \texttt{positive}\} and \{\texttt{delivery}, \texttt{negative}\}.

% -------------------------------------------------------------------
% Difficulties and Challenges
% -------------------------------------------------------------------


% -------------------------------------------------------------------
% Common Approaches
% -------------------------------------------------------------------

\section{Related Works}
Due to broad applications in giving necessary details on different aspects of the sentence or document, ABSA has been extensively researched in various languages. ABSA was first researched and introduced by (Hu and Liu, 2004 \cite{hu2004mining}) in which they only aim to determine product features/aspects that the reviewers have commented on. In the past few years, neural network-based systems have became trending adaptation for ABSA problem as these methods can be trained end-to-end and automatically learn important features (Jiang et al., 2019 \cite{jiang2019challenge}). (Wei and Tao, 2018 \cite{xue2018aspect}) developed Gated Convolutional Network, a model based on convolutional neural networks and gating mechanisms. (Yukun et al., 2018 \cite{ma2018sentic}) proposed Sentic LSTM, an extension of long-short term memory (LSTM) network. (Nguyen and Shirai, 2015 \cite{nguyen2015phrasernn}) introduced recursive neural network approach to make the representation of the target aspect richer by using syntactic information.
In Vietnamese, various methods were proposed in the recent years to address ABSA problem with Vietnamese textual data such as BRNN-CRF architecture (Mai and Le, 2018 \cite{mai2018aspect}), SVM-based model (Thin et al., 2018 \cite{van2018transformation}), Semantic Relation Analysis (Tran and Phan, 2018 \cite{tran2018towards}), semi-supervised learning (Nguyen-Nhat et al., 2019 \cite{nguyen2019one}), etc. 



% -------------------------------------------------------------------
% Contributions and Structure of the Thesis
% -------------------------------------------------------------------
\section{Contributions and Structure of the Report}
The main contribution of this work are:

(i) We build a data set for training and testing the sentiment classification model. Implementation steps include surveying, collecting, normalizing, annotating, calibrating, and analyzing data.

(ii) We propose a multi-label classification model for the second sub-task of ABSA. Word window is used to extract information of specific aspects and various data representation methods to extract features for classifiers. Afterwards, we apply classic models concurrently with Deep Learning adaptation to find out best approach for our situation.

The remainders of the paper is organized as follows. Chapter~\ref{chap:dataset-construction} describes the process of dataset construction: collection, annotation and analysis. The research directions of the ABSA problem relevant to dedicated domain (Technology and Mother \& Baby in detail) is represented in chapter~\ref{chap:model}. Chapter~\ref{chap:experiments} describes the experiments that we illustrates the statistics of all methods considered in each of our model's components and finally, chapter~\ref{chap:conclusion} concludes the topic with our future works.